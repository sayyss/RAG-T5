{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1e19479e-a095-4b30-ad28-9ed5499a3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfe5370-eb45-4541-a964-25bd7ec292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"Word\"\n",
    "w2 = \"Word2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cfbc63-f1f0-4ce3-9ec1-0a64c389a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw1,w2 -> [[0.2,0.2,0.4]\\n         [0.1,0.2,0.4]]\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "w1,w2 -> [[0.2,0.2,0.4]\n",
    "         [0.1,0.2,0.4]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f159ad-8b60-4e8e-8e33-a32ffddb8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural model\n",
    "\n",
    "# 1) Map all words from vocab to a real vector of size m\n",
    "# 2) C matrix -> (len(vocab), m)\n",
    "# 3) all items in C are trainable\n",
    "\n",
    "# Probability function\n",
    "# 1) function g(input sequence of feature vectors(words)) -> maps input sequence to a next possible \n",
    "# word using a conditional probability distribution\n",
    "# output from g -> vector whose ith element estimates probability P(w_t|w_t-1)\n",
    "# g is the neural network\n",
    "\n",
    "# Combine both g and matrix C\n",
    "# 1) function f(sequence_of_vectors(words)) -> g(i, C(w_t-1), C(w_t-n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dab39-d863-4763-88ab-5e3869d3cc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f733004-42ea-4fd4-b179-140ea849776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny = b + Wx + U * tanh(d + Hx)\\n\\nx = concat of all input sequence feature vectors(words)\\nb = biases for W\\nd = biases for H\\nW = direct representation matrix\\nH = hidden layer matrix\\nU = another hidden to output layer matrix\\n\\ny = (Wx + b) + (U * tanh(d+Hx))\\ny =  (1,|V|) +   (1, |V|)\\n     \\ngoes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\\n|V| -> length of vocabuluary\\n\\nthen (1,|V|) -> softmax -> probabilities for each word in vocab\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural model in terms of matrix\n",
    "\"\"\"\n",
    "y = b + Wx + U * tanh(d + Hx)\n",
    "\n",
    "x = concat of all input sequence feature vectors(words)\n",
    "b = biases for W\n",
    "d = biases for H\n",
    "W = direct representation matrix\n",
    "H = hidden layer matrix\n",
    "U = another hidden to output layer matrix\n",
    "\n",
    "y = (Wx + b) + (U * tanh(d+Hx))\n",
    "y =  (1,|V|) +   (1, |V|) \n",
    "     \n",
    "goes to two different models, addition = (1,|V|) + (1, |V|) = (1,|V|)\n",
    "|V| -> length of vocabuluary\n",
    "\n",
    "then (1,|V|) -> softmax -> probabilities for each word in vocab\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aee48ba0-8f80-48b0-bc88-57465faf46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dataset\n",
    "\n",
    "import re\n",
    "\n",
    "words = []\n",
    "\n",
    "with open(\"dataset.txt\",\"r\") as file:\n",
    "    file_content = file.read()\n",
    "    file_content = re.split('; |, |\\*|\\n', file_content)\n",
    "    file_content = re.split(\" \", str(file_content))\n",
    "    words.extend(list(set(file_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc67ce34-7f5c-4ffc-a10d-d07a3e9cc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87632bb4-a0ec-42a0-8627-ca787db35039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18988"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4457bd91-ea30-46d0-84e4-f6250915a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i = {}\n",
    "\n",
    "for i in enumerate(words):\n",
    "    word_to_i[i[1]] = i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1e2d561-3a48-44c8-b3a5-b315ee2846ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_word = {}\n",
    "for i in enumerate(words):\n",
    "    i_to_word[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7afaecb1-96be-4da1-ae3c-1ce005fd9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y labels\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(words)-6):\n",
    "    first_five = words[i:i+5]\n",
    "    next = words[i+5:i+6]\n",
    "\n",
    "    x.append(first_five)\n",
    "    y.append(next[0])\n",
    "    #print(first_five)\n",
    "    #print(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a250fcc6-9b72-45ee-a457-24d8750d22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "train_x = x[:int(len(x)*0.8)]\n",
    "test_x = x[len(train_x)-1:]\n",
    "\n",
    "train_y = y[:int(len(y)*0.8)]\n",
    "test_y = y[len(train_y)-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b03b726b-7239-4860-9b56-50fc51f6fc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15185, 15185, 3798, 3798)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0fff0-c10b-411a-a5d3-bdff8c46c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9219a26a-74f2-4a28-95d7-c39d5e7c6f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHidden units: 50\\nm: 60\\nn: 5\\ndirect: yes\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model config\n",
    "\"\"\"\n",
    "Hidden units: 50\n",
    "m: 60\n",
    "n: 5\n",
    "direct: yes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8129a1b4-f6d4-45d7-bf42-8db5db618a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer:  torch.Size([300, 50])\n",
      "U layer:  torch.Size([50, 18988])\n",
      "Direct representation layer:  torch.Size([300, 18988])\n",
      "C matrix:  torch.Size([18988, 60])\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "# multiple sequence of words as input\n",
    "feature_vector_len = 60\n",
    "hidden_units = 50\n",
    "vocab = len(words)\n",
    "n = 5\n",
    "\n",
    "hidden_layer = torch.randn(n*feature_vector_len, hidden_units, requires_grad=True)\n",
    "U = torch.randn(hidden_units, vocab, requires_grad=True)\n",
    "direct_layer = torch.randn(n*feature_vector_len, vocab, requires_grad=True)\n",
    "C = torch.randn(vocab, feature_vector_len, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([C, direct_layer, hidden_layer, U, d,b], lr=0.01, momentum=0.9)\n",
    "\n",
    "print(\"hidden layer: \", hidden_layer.shape)\n",
    "print(\"U layer: \", U.shape)\n",
    "print(\"Direct representation layer: \", direct_layer.shape)\n",
    "print(\"C matrix: \", C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "71c0516f-b106-4cbe-bb0a-4bf55f44e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequence:  ['smelling', \"slab',\", 'assumed.', 'dining-room', \"'Doran.\"]\n",
      "next word:  shock\n",
      "feature vectors: torch.Size([1, 300])\n",
      "label:  torch.Size([60])\n",
      "\n",
      "Input @ Hidden layer\n",
      "layer 1 output: torch.Size([1, 50])\n",
      "\n",
      " Output from layer 1 @ Output layer\n",
      "layer 2 output: torch.Size([1, 18988])\n",
      "\n",
      " Input @ Direct rep\n",
      "Direct rep output: torch.Size([1, 18988])\n",
      "\n",
      "Final output - layer 2 + direct: torch.Size([1, 18988])\n",
      "\n",
      "softmax output: torch.Size([1, 18988])\n",
      "\n",
      "prediction: sherry',\n",
      "104.25829315185547\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=1)\n",
    "CLE = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_feature_vector(word):\n",
    "    index = word_to_i[word]\n",
    "    return C[index]\n",
    "    \n",
    "print(\"input sequence: \",train_x[0])\n",
    "print(\"next word: \",train_y[0])\n",
    "\n",
    "feature_vectors = torch.stack([get_feature_vector(word) for word in train_x[0]])\n",
    "feature_vectors = torch.cat(torch.unbind(feature_vectors), dim=0)\n",
    "feature_vectors = feature_vectors.view(1,-1)\n",
    "print(\"feature vectors:\", feature_vectors.shape)\n",
    "\n",
    "label = get_feature_vector(train_y[0])\n",
    "print(\"label: \", label.shape)\n",
    "\n",
    "# Hidden layer\n",
    "d = torch.tensor(1.0,requires_grad=True)\n",
    "layer_1_output = torch.matmul(feature_vectors, hidden_layer) + d \n",
    "print(\"\\nInput @ Hidden layer\")\n",
    "print(\"layer 1 output:\", layer_1_output.shape)\n",
    "\n",
    "layer_1_output = tanh(layer_1_output)\n",
    "\n",
    "# Hidden to output layer\n",
    "layer_2_output = torch.matmul(layer_1_output, U)\n",
    "print(\"\\n Output from layer 1 @ Output layer\")\n",
    "print(\"layer 2 output:\", layer_2_output.shape)\n",
    "\n",
    "\n",
    "# Direct representation\n",
    "b = torch.tensor(1.0,requires_grad=True)\n",
    "direct_output = torch.matmul(feature_vectors, direct_layer) + b\n",
    "print(\"\\n Input @ Direct rep\")\n",
    "print(\"Direct rep output:\", direct_output.shape)\n",
    "\n",
    "# Concat\n",
    "final_output = layer_2_output + direct_output\n",
    "print(\"\\nFinal output - layer 2 + direct:\", final_output.shape)\n",
    "\n",
    "# Softmax\n",
    "prob = softmax(final_output)\n",
    "print(\"\\nsoftmax output:\", prob.shape)\n",
    "\n",
    "answer = torch.argmax(prob)\n",
    "print(\"\\nprediction:\", i_to_word[answer.item()])\n",
    "\n",
    "# Loss\n",
    "loss = CLE(final_output, torch.tensor([word_to_i[train_y[0]]]))\n",
    "print(loss.item())\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e55550-185e-4009-ab09-ad216b290016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_x = train_x[:50]\n",
    "batch_y = train_y[:50]\n",
    "\n",
    "epoch = 10\n",
    "for i in range(epoch):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b890b-58a8-42b8-aaa2-1baeb0d78053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5f1f2-b1dd-4448-9301-9544c41b2731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
