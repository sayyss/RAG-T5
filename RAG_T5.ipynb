{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a900d01-17da-43cb-9c9f-10fcca866074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List\n",
    "from langchain_core.runnables.config import run_in_executor\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89969305-1a77-4eb3-bf95-e9a27963234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-T5-738M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404438ec-3d2c-42cb-a62d-610ccf34d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-T5-738M\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bf6455-ee98-4448-b893-cb841de26a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "class T5Embeddings(Embeddings):\n",
    "\n",
    "    def __init__(self, model_name: str = \"MBZUAI/LaMini-T5-738M\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _embed_text(self, text: str) -> List[float]:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.shared(input_ids).mean(dim=1)\n",
    "        \n",
    "        return embeddings.squeeze().cpu().numpy().tolist()\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return [self._embed_text(text) for text in texts]\n",
    "\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self._embed_text(text)\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "            \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = input_ids.input_ids.to(self.device)\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=512)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4962b05e-944b-4338-8676-23c50916d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "          \n",
    "    return faiss_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb8a22-59de-4501-acda-56a5385534d2",
   "metadata": {},
   "source": [
    "### T5 model with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1e551ac-43f3-453c-82d5-553a5fc171d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\") \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "990f16ee-655a-41b7-9826-8a31f5e6b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide information about which of the two key strategies is not used in the first step of GPEFT.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"T5 MODEL WITH T5 EMBEDDINGS\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797c477-84b7-4d38-a8c2-4d70ac1e4a91",
   "metadata": {},
   "source": [
    "### T5 model with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb2931e4-e299-43b2-beec-18efdf7b49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e7b216-f80d-4a93-a044-f32e787f71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article does not provide information about the year when the article was published.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"T5 MODEL WITH LLAMA EMBEDDINGS\"\n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60a112-e7d5-4b8f-a037-57a9d5f0c8ee",
   "metadata": {},
   "source": [
    "### Llama 2 with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9179e27d-adfd-4be6-92d4-d6f589fb7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\")\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e5ed92e-3557-4a06-a0ab-293d44c5f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Incorporate LLM Embeddings: The proposed method, GPEFT, leverages the pre-trained language model (LLM) embeddings to represent textual information in the graph. This allows for more efficient and effective representation learning on text-rich graphs.\n",
      "2. Graph Prompt Encoder: A GNN is used to encode structural information from neighboring nodes into a graph prompt. This enhances the LLM's ability to capture complex relationships between nodes in the graph.\n",
      "3. PEFT Algorithm: The proposed method employs a prefix-tuning and fine-tuning (PTFT) algorithm, which improves the efficiency of the pre-training process. This allows for faster adaptation to new tasks while maintaining performance.\n",
      "4. Efficient Representation Learning: GPEFT optimizes the LLM weights using only minor adjustments, indicating that the pre-trained LLM can be adapted to various tasks with minimal changes.\n",
      "5. Parameter Sensitivity Analysis: The proposed method performs a parameter sensitivity analysis to identify important hyperparameters in GPEFT. This helps to understand the impact of different parameters on the performance of the model.\n",
      "6. Future Work: Expanding GPEFT to handle more complex graph structures, such as dynamic and heterogeneous graphs, is an area of future work. Additionally, exploring other pre-training techniques, such as contrastive learning, to improve the performance of LLMs on graph-structured data is also worth considering.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"LLAMA 2 7B with T5 embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f4ab7-16c8-4ed2-a671-8e0781a4038f",
   "metadata": {},
   "source": [
    "### Llama 2 with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1940411-70c1-4b87-a84c-0e3119b8119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "\n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cb92b-e20c-48b0-a656-d2e4d7d633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"LLAMA 2 7b with llama embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfac1c-ea26-4a56-9a8a-319fa069bba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
