{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a900d01-17da-43cb-9c9f-10fcca866074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List\n",
    "from langchain_core.runnables.config import run_in_executor\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.outputs import GenerationChunk\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89969305-1a77-4eb3-bf95-e9a27963234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-T5-738M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404438ec-3d2c-42cb-a62d-610ccf34d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-T5-738M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-T5-738M\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bf6455-ee98-4448-b893-cb841de26a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "class T5Embeddings(Embeddings):\n",
    "\n",
    "    def __init__(self, model_name: str = \"MBZUAI/LaMini-T5-738M\"):\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _embed_text(self, text: str) -> List[float]:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.shared(input_ids).mean(dim=1)\n",
    "        \n",
    "        return embeddings.squeeze().cpu().numpy().tolist()\n",
    "        \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        return [self._embed_text(text) for text in texts]\n",
    "\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        return self._embed_text(text)\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "            \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = input_ids.input_ids.to(self.device)\n",
    "        \n",
    "        outputs = model.generate(input_ids, max_length=512)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": \"CustomChatModel\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb8a22-59de-4501-acda-56a5385534d2",
   "metadata": {},
   "source": [
    "### T5 model with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e551ac-43f3-453c-82d5-553a5fc171d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\")\n",
    "\n",
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "          \n",
    "    return faiss_index\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990f16ee-655a-41b7-9826-8a31f5e6b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide information about which of the two key strategies is not used in the first step of GPEFT.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"T5 MODEL WITH T5 EMBEDDINGS\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797c477-84b7-4d38-a8c2-4d70ac1e4a91",
   "metadata": {},
   "source": [
    "### T5 model with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb2931e4-e299-43b2-beec-18efdf7b49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "llm = CustomLLM(n=5)\n",
    "\n",
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "          \n",
    "    return faiss_index\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7e7b216-f80d-4a93-a044-f32e787f71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article does not provide information about the year when the article was published.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"T5 MODEL WITH LLAMA EMBEDDINGS\"\n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60a112-e7d5-4b8f-a037-57a9d5f0c8ee",
   "metadata": {},
   "source": [
    "### Llama 2 with T5 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9179e27d-adfd-4be6-92d4-d6f589fb7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = T5Embeddings(\"MBZUAI/LaMini-T5-738M\")\n",
    "\n",
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "          \n",
    "    return faiss_index\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5ed92e-3557-4a06-a0ab-293d44c5f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Use of GNNs for Graph Prompt Encoding: The use of GNNs for encoding structural information from neighboring nodes into a graph prompt is a crucial aspect of SemPool. By incorporating the node features and neighborhood structures, GNNs can learn to encode the structural information effectively.\n",
      "2. Multi-Layer Language Model (MLLM) Pre-training: To pre-train the language model, SemPool utilizes a multi-layer MLLM architecture that is fine-tuned for downstream tasks. The pre-training step enables the language model to learn high-level semantic representations of text data, which are then used for graph prompt encoding.\n",
      "3. Fusion of Semantic Information: SemPool aggregates the semantic information from the whole graph into a single vector through a fusion module. This module combines the node features and neighborhood structures to generate a unified representation of the graph.\n",
      "4. Efficient Representation Learning: To learn efficient representations, SemPool employs an efficient training strategy that utilizes both the pre-training and fine-tuning steps. By optimizing the parameters in these stages, SemPool can learn high-quality representations with minimal computational costs.\n",
      "5. Applicability to Industrial Scenarios: SemPool is designed to be applicable to numerous industrial scenarios involving text-rich graphs. Its lightweight architecture and efficient training strategy make it a promising approach for real-world applications.\n",
      "6. Parameter Sensitivity Analysis: To analyze the sensitivity of SemPool's performance to hyperparameters, an ablation study is conducted. The results demonstrate that minor adjustments to the pre-training weights can significantly affect the performance, but optimizing for a single task requires only minor modifications to the pre-trained weights.\n",
      "7. Future Work Directions: There are several future work directions for SemPool, including exploring different GNN architectures, developing more efficient training strategies, and applying the approach to graph-based recommendation systems and ranking tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"LLAMA 2 7B with T5 embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f4ab7-16c8-4ed2-a671-8e0781a4038f",
   "metadata": {},
   "source": [
    "### Llama 2 with llama embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1940411-70c1-4b87-a84c-0e3119b8119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "embeddings = (\n",
    "    OllamaEmbeddings()\n",
    ")\n",
    "\n",
    "def get_embeddings_from_pdf(pdf_folder_path, embeddings):\n",
    "    documents = []\n",
    "    for file in os.listdir(pdf_folder_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder_path, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            documents.extend(loader.load())\n",
    "            \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    faiss_index = FAISS.from_documents(chunked_documents, embeddings)\n",
    "          \n",
    "    return faiss_index\n",
    "    \n",
    "faiss_index = get_embeddings_from_pdf(\"./prof_1/\", embeddings)\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"\"\"You are an helpful assistant and you try to use the following information to answer the questions given at the end:  \n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593cb92b-e20c-48b0-a656-d2e4d7d633c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passage discusses the effectiveness of large language models (LLMs) at graph representation learning, particularly with the GPEFT pre-training phase. The author provides several observations based on experiments conducted on two different domains:\n",
      "\n",
      "1. Masked language models (e.g., Sentence-BERT) were popularly used for sequence representation, but large language models (e.g., PEFT-LLaMA) have shown improved performance over masked language models in graph representation learning.\n",
      "2. LLMs yield more than 10% improvement over Sentence-BERT in terms of graph representation learning.\n",
      "\n",
      "The author also discusses the limitations of current QA GNN-based approaches, which use external node embeddings to represent the nodes' information. The findings suggest that GNNs rely on the underlying graph statistics to discriminate between correct and incorrect answers.\n",
      "\n",
      "To address these limitations, the author proposes a novel approach called InstructGLM-embeddings, which utilizes a graph neural network (GNN) to encode structural information from neighboring nodes into a graph prompt. The proposed method adapts the GPEFT pre-training phase by adding an additional prompt encoding step.\n",
      "\n",
      "The author concludes that InstructGLM-embeddings can effectively fuse semantic information between natural language and graph, leading to improved QA performance compared to existing approaches.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"LLAMA 2 7b with llama embeddings\" \n",
    "\n",
    "query = \"how to utilize a graph neural network (GNN) to encode structural infor-\"\\\n",
    "\"mation from neighboring nodes into a graph prompt?\"\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfac1c-ea26-4a56-9a8a-319fa069bba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
